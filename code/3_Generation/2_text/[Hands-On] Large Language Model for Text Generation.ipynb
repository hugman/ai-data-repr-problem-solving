{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d11bff3-42a1-46aa-ae72-83a7600349f7",
   "metadata": {},
   "source": [
    "# [Hands-On] Large Language Model for Text Generation\n",
    "\n",
    "Author: Your Name or Organization\n",
    "\n",
    "> Educational Purpose\n",
    "\n",
    "In this tutorial, we'll explore how to load and run a **4-bit quantized** Falcon-7B Instruct model using Hugging Face `transformers` and `bitsandbytes`. This approach can significantly reduce GPU/CPU memory usage while still delivering relatively strong performance for text generation tasks.\n",
    "\n",
    "We'll walk through:\n",
    "1. **Installing** necessary libraries  \n",
    "2. **Loading** the Falcon-7B Instruct model in 4-bit precision  \n",
    "3. **Generating** text responses from sample prompts  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41312457-002f-47f7-aa94-175585a579b7",
   "metadata": {},
   "source": [
    "## 1) Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ededa0e-d524-4bbf-825e-8b46de6dc75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\hugman\\anaconda3\\envs\\book_ai\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\hugman\\anaconda3\\envs\\book_ai\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\hugman\\anaconda3\\envs\\book_ai\\lib\\site-packages (0.45.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\hugman\\anaconda3\\envs\\book_ai\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\hugman\\anaconda3\\envs\\book_ai\\lib\\site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hugman\\anaconda3\\envs\\book_ai\\lib\\site-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hugman\\anaconda3\\envs\\book_ai\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hugman\\anaconda3\\envs\\book_ai\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hugman\\anaconda3\\envs\\book_ai\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\hugman\\anaconda3\\envs\\book_ai\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\hugman\\anaconda3\\envs\\book_ai\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\hugman\\anaconda3\\envs\\book_ai\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hugman\\anaconda3\\envs\\book_ai\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\hugman\\anaconda3\\envs\\book_ai\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\hugman\\anaconda3\\envs\\book_ai\\lib\\site-packages (from accelerate) (2.6.0+cu126)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hugman\\anaconda3\\envs\\book_ai\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hugman\\anaconda3\\envs\\book_ai\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\hugman\\anaconda3\\envs\\book_ai\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hugman\\anaconda3\\envs\\book_ai\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\hugman\\anaconda3\\envs\\book_ai\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hugman\\anaconda3\\envs\\book_ai\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hugman\\anaconda3\\envs\\book_ai\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hugman\\anaconda3\\envs\\book_ai\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hugman\\anaconda3\\envs\\book_ai\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hugman\\anaconda3\\envs\\book_ai\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hugman\\anaconda3\\envs\\book_ai\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hugman\\anaconda3\\envs\\book_ai\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8717ae6-bd04-4d0f-84f4-79d786b092ca",
   "metadata": {},
   "source": [
    "- **transformers**: Core library for state-of-the-art NLP models.\n",
    "- **accelerate**: Facilitates multi-GPU/distributed training or inference.\n",
    "- **bitsandbytes**: Enables 8-bit or 4-bit quantization for model weights, saving memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981e7300-8a49-4ac3-b325-4574b8ea1e89",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2) Imports & Basic Setup\n",
    "\n",
    "We import PyTorch, along with classes from `transformers` that help us load the tokenizer, model, and configuration settings for 4-bit quantization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60a0156f-1934-4b81-b20e-71102ee88180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4833a3f5-f72e-4897-8f64-69c4ba60ca45",
   "metadata": {},
   "source": [
    "- `AutoTokenizer`: Automatically loads the correct tokenizer for our specified model.\n",
    "- `AutoModelForCausalLM`: Loads a causal language model (decoder-only) for text generation.\n",
    "- `BitsAndBytesConfig`: Configuration class that specifies 4-bit quantization settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595530c7-9d35-4197-95ec-d5aadf7ad3fa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3) Load the Falcon-7B-Instruct Model in 4-bit Precision\n",
    "\n",
    "Below is a helper function that:\n",
    "1. Defines the model name (`tiiuae/falcon-7b-instruct`).\n",
    "2. Sets `BitsAndBytesConfig` to load the model in 4-bit precision.\n",
    "3. Instantiates the tokenizer and model from the Hugging Face Hub.\n",
    "4. Returns both the tokenizer and model for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45e6b1f7-83d4-474f-ad0b-fed2f39c914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_falcon_7b_instruct_4bit():\n",
    "    \"\"\"\n",
    "    Loads the Falcon-7B-Instruct model using 4-bit quantization and returns\n",
    "    the tokenizer and model objects.\n",
    "    \"\"\"\n",
    "    model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "    \n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quant_config,\n",
    "        device_map=\"auto\",      # Automatically places model layers on available GPUs\n",
    "        trust_remote_code=True  # Trust custom code from the model repo\n",
    "    )\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    return tokenizer, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4220dad-cb47-4b92-ac7f-2408f506a2ff",
   "metadata": {},
   "source": [
    "**Note**:  \n",
    "- `device_map=\"auto\"` ensures the model is loaded onto your available GPUs/CPU.  \n",
    "- 4-bit quantization helps reduce memory usage but may slightly affect generation quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283133d1-c58c-4cb7-a3bc-5f4dcc5fb4fb",
   "metadata": {},
   "source": [
    "---\n",
    "## 4) Define a Text Generation Function\n",
    "\n",
    "We create a function `generate_text` which:\n",
    "1. Tokenizes the prompt into `input_ids`.\n",
    "2. Uses `model.generate()` to produce an output sequence.\n",
    "3. Decodes the generated tokens back to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92865773-a307-4f95-bbab-daabcb595f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(tokenizer, model, prompt, max_new_tokens=80, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Generates text from a given prompt using the provided tokenizer/model.\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46e9ff6-adb0-48ed-9860-c6b467c89e7c",
   "metadata": {},
   "source": [
    "**Parameters**:\n",
    "- `max_new_tokens`: Maximum number of tokens to generate.\n",
    "- `temperature`: Controls randomness; higher means more diverse but potentially less coherent text.\n",
    "- `top_p`: Used for nucleus sampling; considers tokens within the cumulative probability.\n",
    "- `repetition_penalty`: Penalizes repeated tokens to reduce looping outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa14b6f2-0f68-4af0-baeb-8dc36b65b39e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aee0534-865c-4c15-a812-46b037f13c6c",
   "metadata": {},
   "source": [
    "## 5) Model Preparation\n",
    "- Load the 4-bit quantized Falcon-7B-Instruct model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33935429-f8d3-4f2d-83a2-0f74315805b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.35s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer, model = load_falcon_7b_instruct_4bit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a89e33-8e3d-4514-ba43-ce97c28a376b",
   "metadata": {},
   "source": [
    "## 6) Demo: Generate Text for Individual Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe39d28-dd9f-43d8-8ca5-62b88a032903",
   "metadata": {},
   "source": [
    "### Case A: Poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdc8c320-0cb4-4132-9e50-92536324b41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " ### Instruction:\n",
      "Write a short poem about sunrise on a beach.\n",
      "### Response:\n",
      "The sun rises on the horizon,\n",
      "Splashing its golden hues across the sky.\n",
      "The waves of the sea shimmer in the light,\n",
      "As the horizon glows with a heavenly sight.\n",
      "The air is filled with a new-born energy,\n",
      "As the day slowly starts to bring serenity\n"
     ]
    }
   ],
   "source": [
    "# Case A: Poem\n",
    "# Prompt:\n",
    "prompt_case_a = \"\"\"### Instruction:\n",
    "Write a short poem about sunrise on a beach.\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "output_a = generate_text(tokenizer, model, prompt_case_a, max_new_tokens=60, temperature=0.7)\n",
    "print(\"Generated Text:\\n\", output_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e87b64a-c2a4-4f74-b8fb-d1d025b18e33",
   "metadata": {},
   "source": [
    "- The model successfully follows the poetic prompt, describing the sunrise visually.  \n",
    "- Even in 4-bit quantization, the model retains its ability to generate coherent and creative text.  \n",
    "- It may occasionally produce repeated words, but overall captures the intended imagery.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff68b4fc-5803-4ea1-8911-116432c90390",
   "metadata": {},
   "source": [
    "### Case B: Broad Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b366a9ee-aa07-47a5-9de9-70117775ca64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " ### Instruction:\n",
      "Explain who Albert Einstein was and why he is famous.\n",
      "### Response:\n",
      "Albert Einstein (14 March 1879 – 18 April 1955) was a German-born theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics. His work is also recognized in the field of quantum theory, cosmology, and astrophysics\n"
     ]
    }
   ],
   "source": [
    "# Prompt:\n",
    "prompt_case_b = \"\"\"### Instruction:\n",
    "Explain who Albert Einstein was and why he is famous.\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "output_b = generate_text(tokenizer, model, prompt_case_b, max_new_tokens=60, temperature=0.7)\n",
    "print(\"Generated Text:\\n\", output_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8487593e-c19d-412f-8e44-a977be0fd69f",
   "metadata": {},
   "source": [
    "- Here, the model leverages its built-in knowledge base to explain Albert Einstein’s contributions (relativity, E=mc², quantum theory).  \n",
    "- The generated text is succinct and factually accurate in broad strokes.  \n",
    "- 4-bit quantization does not seem to hinder its factual output for well-known topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffd38e3-d366-4a0b-88e9-20b1dbe1c04e",
   "metadata": {},
   "source": [
    "### Case C: Simple Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "156933ab-3db9-49f3-950b-071338225e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " ### Instruction:\n",
      "You have 5 apples. You give 2 apples to your friend and then buy 3 more apples. How many apples do you have now?\n",
      "### Response:\n",
      "You have 8 apples now.\n"
     ]
    }
   ],
   "source": [
    "# Prompt:\n",
    "prompt_case_c = \"\"\"### Instruction:\n",
    "You have 5 apples. You give 2 apples to your friend and then buy 3 more apples. How many apples do you have now?\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "output_c = generate_text(tokenizer, model, prompt_case_c, max_new_tokens=30, temperature=0.01)\n",
    "print(\"Generated Text:\\n\", output_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7edd2a-597c-4d0d-bd74-b7e44df09fbb",
   "metadata": {},
   "source": [
    "- The model incorrectly calculated 5 - 2 + 3 = 8 when the correct answer is 6 apples\n",
    "- This happens because LLMs are pattern-matching text predictors, not calculators - they try to predict the next likely tokens rather than perform actual math\n",
    "- Even with low temperature (0.01), we get incorrect math because reducing randomness doesn't improve the model's fundamental ability to calculate\n",
    "- For reliable mathematical calculations, traditional programming methods should be used instead of LLMs\n",
    "\n",
    "This example shows why LLMs, while powerful for language tasks, aren not sometimes suitable for precise arithmetic operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f44756-5faa-40a2-aab3-2ae168e43556",
   "metadata": {},
   "source": [
    "### Case D: Summarize Pride and Prejudice in Two Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d4ceae98-d74b-4544-be02-5f2eb0bc6f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " ### Instruction:\n",
      "Please summarize the main events of 'Pride and Prejudice' in two sentences. Focus on the relationships between the characters.\n",
      "### Response:\n",
      "Elizabeth Bennet, a witty and intelligent young woman, is introduced to wealthy and proud Mr. Darcy. They overcome their pride and prejudices, eventually realizing they are perfect for one another.\n"
     ]
    }
   ],
   "source": [
    "# Prompt:\n",
    "prompt_case_d = \"\"\"### Instruction:\n",
    "Please summarize the main events of 'Pride and Prejudice' in two sentences. Focus on the relationships between the characters.\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "output_d = generate_text(tokenizer, model, prompt_case_d, max_new_tokens=60, temperature=0.7)\n",
    "print(\"Generated Text:\\n\", output_d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33827a9e-def0-4427-ba1c-5866b4ba85ce",
   "metadata": {},
   "source": [
    "- The model does manage to summarize in roughly two sentences, focusing on the relationships, particularly between Elizabeth Bennet and Mr. Darcy.  \n",
    "- It highlights the key themes (love, social class, individual growth).  \n",
    "- For strict two-sentence outputs, you may need to adjust the model’s temperature or post-process the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5e01c3-31f1-4187-b0dc-c90f6cfc009d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this hands-on tutorial, we demonstrated how to:\n",
    "1. **Install** the necessary libraries (`transformers`, `accelerate`, `bitsandbytes`).\n",
    "2. **Load** a Falcon-7B-Instruct model with **4-bit quantization** to save memory.\n",
    "3. **Generate** text responses from sample instructions or prompts.\n",
    "\n",
    "With this setup:\n",
    "- You can handle larger models on limited hardware, although some quality trade-offs might appear.\n",
    "- By tweaking hyperparameters (temperature, top_p, repetition_penalty, etc.) or fine-tuning the model further (using methods such as LoRA, PEFT), you can adapt the system to more specialized tasks or improve output quality.\n",
    "\n",
    "Feel free to experiment with additional prompts or integrate this approach into larger applications!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book_ai",
   "language": "python",
   "name": "book_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
